{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def create_classifiers(class_weight: str = \"balanced\"):\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight=class_weight),\n",
    "        \"SVM with RBF Kernel\": SVC(kernel=\"rbf\", class_weight=class_weight, probability=True),\n",
    "        \"SVM with Linear Kernel\": SVC(kernel=\"linear\", class_weight=class_weight, probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight=class_weight),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(class_weight=class_weight),\n",
    "        \"k-NN\": KNeighborsClassifier(),\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        \"Logistic Regression\": {\"C\": [0.001, 0.01, 0.1, 1, 10]},\n",
    "        \"SVM with RBF Kernel\": {\"C\": [0.1, 1, 10], \"gamma\": [0.01, 0.1, 1]},\n",
    "        \"SVM with Linear Kernel\": {\"C\": [0.1, 1, 10]},\n",
    "        \"Random Forest\": {\n",
    "            \"n_estimators\": [10, 50, 100],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "        },\n",
    "        \"Decision Tree\": {\"max_depth\": [None, 10, 20, 30]},\n",
    "        \"k-NN\": {\"n_neighbors\": [3, 5, 7, 9]},\n",
    "    }\n",
    "\n",
    "    return classifiers, param_grids\n",
    "\n",
    "\n",
    "def run_training_loop(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    task=\"a\",\n",
    "    class_weight: str = \"balanced\",\n",
    "    verbose: bool = False,\n",
    "    average=\"micro\",\n",
    "):\n",
    "    # Define classifiers\n",
    "    classifiers, param_grid = create_classifiers(class_weight=class_weight)\n",
    "\n",
    "    # Stratified K-Fold Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    # Metrics storage\n",
    "    results = {\n",
    "        name: {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1_score\": [],\n",
    "            \"f1_macro\": [],\n",
    "        }\n",
    "        for name in classifiers.keys()\n",
    "    }\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    aggregate_conf_matrix_train = {name: np.zeros((n_classes, n_classes)) for name in classifiers.keys()}\n",
    "\n",
    "    best_params_models = {classifier: [] for classifier in classifiers.keys()}\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # re init classifiers to prevent data leakage from previous fold\n",
    "        classifiers, param_grid = create_classifiers(class_weight=class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "            search_clf = RandomizedSearchCV(clf, param_grid[name], scoring=\"f1_macro\", cv=3, n_jobs=-1)\n",
    "\n",
    "            # Train the classifier with the best hyperparameters\n",
    "\n",
    "            search_clf.fit(X_cv, y_cv)\n",
    "\n",
    "            best_params = search_clf.best_params_\n",
    "            best_params_models[name].append(best_params)\n",
    "            clf = search_clf.best_estimator_\n",
    "            # Train the classifier\n",
    "            clf.fit(X_cv, y_cv)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = clf.predict(X_val)\n",
    "\n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            precision = precision_score(y_val, y_pred, average=average)\n",
    "            recall = recall_score(y_val, y_pred, average=average)\n",
    "            f1 = f1_score(y_val, y_pred, average=average)\n",
    "            f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "            conf_matrix = confusion_matrix(y_val, y_pred, labels=np.unique(y_train))\n",
    "            aggregate_conf_matrix_train[name] += conf_matrix\n",
    "\n",
    "            # Store metrics\n",
    "            results[name][\"accuracy\"].append(acc)\n",
    "            results[name][\"precision\"].append(precision)\n",
    "            results[name][\"recall\"].append(recall)\n",
    "            results[name][\"f1_score\"].append(f1)\n",
    "            results[name][\"f1_macro\"].append(f1_macro)\n",
    "\n",
    "    return results, aggregate_conf_matrix_train, best_params_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_variance(X_train, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(\"Sum of the two PCS\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "\n",
    "def get_distribution(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "\n",
    "def standard_scale(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return pd.DataFrame(data_scaled, columns=data.columns, index=data.index)\n",
    "\n",
    "\n",
    "def plot_boxplots(results, y):\n",
    "    accuracy_data = []\n",
    "    precision_data = []\n",
    "    recall_data = []\n",
    "    f1_data = []\n",
    "    f1_macro = []\n",
    "\n",
    "    class_names = sorted(set(y))\n",
    "\n",
    "    for model, metrics in results.items():\n",
    "        # Accuracy\n",
    "        for acc in metrics[\"accuracy\"]:\n",
    "            accuracy_data.append([model, acc])\n",
    "        for f1_mac in metrics[\"f1_macro\"]:\n",
    "            f1_macro.append([model, f1_mac])\n",
    "\n",
    "        # Precision and Recall for each class\n",
    "        for fold in range(len(metrics[\"precision\"])):\n",
    "            for class_idx, prec in enumerate(metrics[\"precision\"][fold]):\n",
    "                precision_data.append([model, class_names[class_idx], prec])\n",
    "            for class_idx, rec in enumerate(metrics[\"recall\"][fold]):\n",
    "                recall_data.append([model, class_names[class_idx], rec])\n",
    "            for class_idx, f1 in enumerate(metrics[\"f1_score\"][fold]):\n",
    "                f1_data.append([model, class_names[class_idx], f1])\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    accuracy_df = pd.DataFrame(accuracy_data, columns=[\"Model\", \"Accuracy\"])\n",
    "    precision_df = pd.DataFrame(precision_data, columns=[\"Model\", \"Class\", \"Precision\"])\n",
    "    recall_df = pd.DataFrame(recall_data, columns=[\"Model\", \"Class\", \"Recall\"])\n",
    "    f1_df = pd.DataFrame(f1_data, columns=[\"Model\", \"Class\", \"F1 Score\"])\n",
    "    f1_macro_df = pd.DataFrame(f1_macro, columns=[\"Model\", \"F1 Score\"])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Boxplot for Accuracy\n",
    "    sns.boxplot(x=\"Model\", y=\"Accuracy\", data=accuracy_df, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title(\"Accuracy Comparison\")\n",
    "    axs[0, 0].set_ylim(0, 1)\n",
    "    labels = axs[0, 0].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "\n",
    "    # Boxplot for Precision\n",
    "    sns.boxplot(x=\"Model\", y=\"Precision\", hue=\"Class\", data=precision_df, ax=axs[0, 1])\n",
    "    axs[0, 1].set_title(\"Precision Comparison by Class\")\n",
    "    axs[0, 1].set_ylim(0, 1)\n",
    "    labels = axs[0, 1].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[0, 1].legend(title=\"Class\", bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    # Boxplot for Recall\n",
    "    sns.boxplot(x=\"Model\", y=\"Recall\", hue=\"Class\", data=recall_df, ax=axs[1, 0])\n",
    "    axs[1, 0].set_title(\"Recall Comparison by Class\")\n",
    "    axs[1, 0].set_ylim(0, 1)\n",
    "    labels = axs[1, 0].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[1, 0].legend(title=\"Class\", bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    # Boxplot for F1 Score\n",
    "    sns.boxplot(x=\"Model\", y=\"F1 Score\", hue=\"Class\", data=f1_df, ax=axs[1, 1])\n",
    "    axs[1, 1].set_title(\"F1 Score Comparison by Class\")\n",
    "    axs[1, 1].set_ylim(0, 1)\n",
    "    labels = axs[1, 1].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[1, 1].legend(title=\"Class\", bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot f1 micro\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(15, 12))\n",
    "    sns.boxplot(x=\"Model\", y=\"F1 Score\", data=f1_macro_df, ax=axs)\n",
    "    axs.set_title(\"F1 Micro Comparison\")\n",
    "    axs.set_ylim(0, 1)\n",
    "    labels = axs.get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pair_and_f1_micro(results):\n",
    "    accuracies = {model: data[\"accuracy\"] for model, data in results.items()}\n",
    "    f1_score = {model: np.array(data[\"f1_macro\"]) for model, data in results.items()}\n",
    "\n",
    "    model_pairs = list(itertools.combinations(accuracies.keys(), 2))\n",
    "\n",
    "    # Calculate differences between pairs of models\n",
    "    differences = {}\n",
    "    for model1, model2 in model_pairs:\n",
    "        differences[f\"{model1} vs {model2}\"] = [acc1 - acc2 for acc1, acc2 in zip(accuracies[model1], accuracies[model2])]\n",
    "\n",
    "    # Convert to a DataFrame for easier plotting\n",
    "    accuracy_diff_df = pd.DataFrame(differences)\n",
    "\n",
    "    differences_f1 = {}\n",
    "    for model1, model2 in model_pairs:\n",
    "        differences_f1[f\"{model1} vs {model2}\"] = [acc1 - acc2 for acc1, acc2 in zip(f1_score[model1], f1_score[model2])]\n",
    "\n",
    "        # Convert to a DataFrame for easier plotting\n",
    "    f1_diff_df = pd.DataFrame(differences_f1)\n",
    "\n",
    "    # Convert F1 scores to a DataFrame for plotting\n",
    "    f1_scores_df = pd.DataFrame(f1_score)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 18))\n",
    "\n",
    "    # # Plot accuracy differences\n",
    "    # sns.boxplot(data=accuracy_diff_df, ax=axes[0])\n",
    "    # axes[0].set_title('Boxplot of Accuracy Differences Between Models')\n",
    "    # axes[0].set_ylabel('Difference in Accuracy')\n",
    "    # axes[0].set_xticklabels(accuracy_diff_df.columns, rotation=90)\n",
    "\n",
    "    # Plot F1 score differences\n",
    "    sns.boxplot(data=f1_diff_df, ax=axes[0])\n",
    "    axes[0].set_title(\"Boxplot of F1 Score Differences Between Models\")\n",
    "    axes[0].set_ylabel(\"Difference in F1 Score\")\n",
    "    axes[0].set_xticklabels(f1_diff_df.columns, rotation=90)\n",
    "\n",
    "    # Plot F1 scores\n",
    "    sns.boxplot(data=f1_scores_df, ax=axes[1])\n",
    "    axes[1].set_title(\"Boxplot of F1 Scores for Each Model\")\n",
    "    axes[1].set_ylabel(\"F1 Score\")\n",
    "    axes[1].set_xticklabels(f1_scores_df.columns, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matricies(aggregate_conf_matrix_train: dict, aggregate_conf_matrix_test: dict, y):\n",
    "    class_names = sorted(set(y))\n",
    "\n",
    "    # Create a figure for train confusion matrices\n",
    "    fig_train, axs_train = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    for i, (model, conf_matrix) in enumerate(aggregate_conf_matrix_train.items()):\n",
    "        sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=True,\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            ax=axs_train[i // 2, i % 2],\n",
    "        )\n",
    "        axs_train[i // 2, i % 2].set_title(f\"Train Confusion Matrix - {model}\")\n",
    "        axs_train[i // 2, i % 2].set_xlabel(\"Predicted\")\n",
    "        axs_train[i // 2, i % 2].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a figure for test confusion matrices\n",
    "    fig_test, axs_test = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    for i, (model, conf_matrix) in enumerate(aggregate_conf_matrix_test.items()):\n",
    "        sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=True,\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            ax=axs_test[i // 2, i % 2],\n",
    "        )\n",
    "        axs_test[i // 2, i % 2].set_title(f\"Test Confusion Matrix - {model}\")\n",
    "        axs_test[i // 2, i % 2].set_xlabel(\"Predicted\")\n",
    "        axs_test[i // 2, i % 2].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    # Iterate through each key in dict1\n",
    "    for key in dict1.keys():\n",
    "        if key in dict2:\n",
    "            merged_dict[key] = {}\n",
    "            for subkey in dict1[key]:\n",
    "                # Check if the subkey exists in both dictionaries\n",
    "                if subkey in dict2[key]:\n",
    "                    # For accuracy, just append the list\n",
    "                    if subkey == \"accuracy\" or subkey == \"f1_micro\":\n",
    "                        merged_dict[key][subkey] = dict1[key][subkey] + dict2[key][subkey]\n",
    "                    else:\n",
    "                        # For arrays, append the lists of arrays\n",
    "                        merged_dict[key][subkey] = dict1[key][subkey] + dict2[key][subkey]\n",
    "                else:\n",
    "                    # Handle the case where a subkey exists in dict1 but not in dict2\n",
    "                    merged_dict[key][subkey] = dict1[key][subkey]\n",
    "        else:\n",
    "            # Handle the case where a key exists in dict1 but not in dict2\n",
    "            merged_dict[key] = dict1[key]\n",
    "    # Add any keys from dict2 that are not in dict1\n",
    "    for key in dict2.keys():\n",
    "        if key not in dict1:\n",
    "            merged_dict[key] = dict2[key]\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def run_train_multiple_testing(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    task=\"a\",\n",
    "    class_weight: str = \"balanced\",\n",
    "    verbose: bool = False,\n",
    "    average=\"micro\",\n",
    "    do_smote=False,\n",
    "):\n",
    "    if do_smote:\n",
    "        smote = SMOTE()\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    results, aggregated_conf_matrix_train, best_params = run_training_loop(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        task=task,\n",
    "        class_weight=class_weight,\n",
    "        verbose=verbose,\n",
    "        average=average,\n",
    "    )\n",
    "    return results, aggregated_conf_matrix_train, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Fish3.txt\", delimiter=\" \")\n",
    "\n",
    "X = data.drop(columns=[\"Species\"])\n",
    "y = data[\"Species\"]\n",
    "\n",
    "print(\"Pre removal: \", len(X))\n",
    "\n",
    "\n",
    "X = X[(X > 0).all(1)]\n",
    "\n",
    "print(\"Post removal: \", len(X), \"Removed: \", len(data) - len(X))\n",
    "\n",
    "y = y.loc[X.index]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = standard_scale(X_train)\n",
    "X_test = standard_scale(X_test)\n",
    "\n",
    "print_pca_variance(X_train, n_components=2)\n",
    "get_distribution(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_train = []\n",
    "all_best_params = []\n",
    "all_conf_matrix_train = {}\n",
    "\n",
    "for _ in range(30):\n",
    "    X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "    (\n",
    "        results_train_temp,\n",
    "        conf_train,\n",
    "        best_params,\n",
    "    ) = run_train_multiple_testing(\n",
    "        X_train_resampled,\n",
    "        y_train_resampled,\n",
    "        task=\"a\",\n",
    "        class_weight=\"balanced\",\n",
    "        average=None,\n",
    "    )\n",
    "    all_results_train.append(results_train_temp)\n",
    "    all_best_params.append(best_params)\n",
    "    for key, value in conf_train.items():\n",
    "        if key in all_conf_matrix_train:\n",
    "            all_conf_matrix_train[key] += value\n",
    "        else:\n",
    "            all_conf_matrix_train[key] = value\n",
    "\n",
    "\n",
    "results_train = all_results_train[0]\n",
    "\n",
    "for i in range(1, len(all_results_train)):\n",
    "    results_train = merge_dictionaries(results_train, all_results_train[i])\n",
    "\n",
    "\n",
    "for key, value in all_conf_matrix_train.items():\n",
    "    all_conf_matrix_train[key] = value / value.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "best_params = {}\n",
    "for res in all_best_params:\n",
    "    for key, value in res.items():\n",
    "        if key in best_params:\n",
    "            best_params[key] += value\n",
    "        else:\n",
    "            best_params[key] = value\n",
    "\n",
    "\n",
    "params_list = best_params\n",
    "\n",
    "final_params = {}\n",
    "for model, params in params_list.items():\n",
    "    # Convert each parameter combination into a string\n",
    "    param_strings = [str(param) for param in params]\n",
    "\n",
    "    # Use Counter to count the frequency of each parameter combination\n",
    "    counter = Counter(param_strings)\n",
    "\n",
    "    # Get the most common parameter combination\n",
    "    most_common_param = counter.most_common(1)[0]\n",
    "\n",
    "    print(f\"Model: {model}, Most common parameter combination: {most_common_param}\")\n",
    "\n",
    "    final_params[model] = eval(most_common_param[0])\n",
    "\n",
    "\n",
    "# Train the models with the best hyperparameters\n",
    "test_conf_matrix = {}\n",
    "classifiers = create_classifiers()[0]\n",
    "classification_reports = {}\n",
    "for model, clf in classifiers.items():\n",
    "    clf.set_params(**final_params[model])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"Model: {model}, Accuracy: {acc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n",
    "    classification_reports[model] = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "    test_conf_matrix[model] = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the aggregated results\n",
    "aggregated_results = []\n",
    "\n",
    "# Iterate over the classification reports\n",
    "for model_name, report in classification_reports.items():\n",
    "    # Extract the relevant metrics from the report\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    macro_avg_precision = report[\"macro avg\"][\"precision\"]\n",
    "    macro_avg_recall = report[\"macro avg\"][\"recall\"]\n",
    "    macro_avg_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "    weighted_avg_precision = report[\"weighted avg\"][\"precision\"]\n",
    "    weighted_avg_recall = report[\"weighted avg\"][\"recall\"]\n",
    "    weighted_avg_f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    # Create a dictionary to store the aggregated results for this model\n",
    "    model_results = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Macro Avg Precision\": macro_avg_precision,\n",
    "        \"Macro Avg Recall\": macro_avg_recall,\n",
    "        \"Macro Avg F1\": macro_avg_f1,\n",
    "        \"Weighted Avg Precision\": weighted_avg_precision,\n",
    "        \"Weighted Avg Recall\": weighted_avg_recall,\n",
    "        \"Weighted Avg F1\": weighted_avg_f1,\n",
    "    }\n",
    "\n",
    "    # Append the model results to the aggregated results list\n",
    "    aggregated_results.append(model_results)\n",
    "\n",
    "# Create a Pandas DataFrame from the aggregated results\n",
    "aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "# Print the aggregated table\n",
    "aggregated_df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pair_and_f1_micro(results_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matricies(all_conf_matrix_train, test_conf_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(results_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "def create_classifiers(class_weight: str):\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight=class_weight),\n",
    "        \"SVM with RBF Kernel\": SVC(kernel=\"rbf\", class_weight=class_weight),\n",
    "        \"SVM with Linear Kernel\": SVC(kernel=\"linear\", class_weight=class_weight),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight=class_weight),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(class_weight=class_weight),\n",
    "        \"k-NN\": KNeighborsClassifier(n_neighbors=3),\n",
    "    }\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def feature_selection_permutation_importance(X, y, clf, threshold = 0.1):\n",
    "    temp_clf = clone(clf)\n",
    "    temp_clf.fit(X, y)\n",
    "\n",
    "    result = permutation_importance(temp_clf, X, y, n_repeats=10, random_state=0).importances_mean\n",
    "    result = result / sum(result) #normalize \n",
    "    return result > threshold\n",
    "\n",
    "\n",
    "def feature_selection_linear(X, y, clf):\n",
    "    selector = RFECV(clf, step=1, cv=5, n_jobs=-1)\n",
    "    selector.fit(X_train_resampled, y_train_resampled)\n",
    "    return selector.support_\n",
    "\n",
    "\n",
    "def feature_selection_tree_importances(X, y, clf):\n",
    "    temp_clf = clone(clf)\n",
    "    selector = SelectFromModel(temp_clf)\n",
    "    selector.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "    return selector.get_support()  \n",
    "\n",
    "\n",
    "def run_training_loop_features(X_train, y_train, class_weight=\"balanced\", average=\"micro\"):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    classifiers = create_classifiers(class_weight)\n",
    "    results = {name: {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1_score\": [], \"f1_macro\": [], \"selected_features\": [], \"correctly_labeled\": [], \"mislabeled\": []} for name in classifiers.keys()}\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        classifiers = create_classifiers(class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "\n",
    "            # Select the appropriate feature selection method\n",
    "            if name in [\"k-NN\", \"SVM with RBF Kernel\"]:\n",
    "                feature_selector = feature_selection_permutation_importance\n",
    "            elif name in [\"SVM with Linear Kernel\", \"Logistic Regression\"]:\n",
    "                feature_selector = feature_selection_linear\n",
    "            else:\n",
    "                feature_selector = feature_selection_tree_importances\n",
    "\n",
    "            selected_features = feature_selector(X_cv, y_cv, clf)\n",
    "\n",
    "            X_cv_fs = X_cv.iloc[:, selected_features]\n",
    "            X_val_fs = X_val.iloc[:, selected_features]\n",
    "\n",
    "            clf.fit(X_cv_fs, y_cv)\n",
    "            y_pred = clf.predict(X_val_fs)\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            precision = precision_score(y_val, y_pred, average=average)\n",
    "            recall = recall_score(y_val, y_pred, average=average)\n",
    "            f1 = f1_score(y_val, y_pred, average=average)\n",
    "            f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "            if name not in results:\n",
    "                results[name] = {\n",
    "                    \"accuracy\": [],\n",
    "                    \"precision\": [],\n",
    "                    \"recall\": [],\n",
    "                    \"f1_score\": [],\n",
    "                    \"f1_macro\": [],\n",
    "                    \"selected_features\": [],\n",
    "                    \"correctly_labeled\": [],\n",
    "                    \"mislabeled\": [],\n",
    "                }\n",
    "\n",
    "            results[name][\"accuracy\"].append(acc)\n",
    "            results[name][\"precision\"].append(precision)\n",
    "            results[name][\"recall\"].append(recall)\n",
    "            results[name][\"f1_score\"].append(f1)\n",
    "            results[name][\"f1_macro\"].append(f1_macro)\n",
    "            results[name][\"selected_features\"].append(selected_features)\n",
    "\n",
    "            correct_mask = y_val == y_pred\n",
    "            num_correctly_labeled = correct_mask.sum()\n",
    "            num_mislabeled = (~correct_mask).sum()\n",
    "\n",
    "            results[name][\"correctly_labeled\"].append(num_correctly_labeled)\n",
    "            results[name][\"mislabeled\"].append(num_mislabeled)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_class_feature_selection(X_train, y_train, class_weight=\"balanced\", average=\"micro\"):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    classifiers = create_classifiers(class_weight)\n",
    "    results = {name: {\"class_features\": []} for name in classifiers.keys()}\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        classifiers = create_classifiers(class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "\n",
    "            # Select the appropriate feature selection method\n",
    "            if name != \"Logistic Regression\":\n",
    "                continue\n",
    "\n",
    "            clf.fit(X_cv, y_cv)\n",
    "\n",
    "            features = clf.coef_\n",
    "\n",
    "\n",
    "            results[name][\"class_features\"].append(features)\n",
    "            \n",
    "            \n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_training_loop_features_misclass(X_train, y_train, class_weight=\"balanced\", average=\"micro\"):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    classifiers = create_classifiers(class_weight)\n",
    "    results = {name: {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1_score\": [], \"f1_macro\": [], \"selected_features\": [], \"correctly_labeled\": [], \"mislabeled\": []} for name in classifiers.keys()}\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        classifiers = create_classifiers(class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "            \n",
    "            \n",
    "            clf.fix(X_cv, y_cv)\n",
    "            y_pred = clf.predict(X_val)\n",
    "            y_pred_train = clf.predict(X_cv)\n",
    "            \n",
    "            correct_mask = y_val == y_pred\n",
    "            correct_mask_train = y_cv == y_pred_train\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            # Select the appropriate feature selection method\n",
    "            if name in [\"k-NN\", \"SVM with RBF Kernel\"]:\n",
    "                feature_selector = feature_selection_permutation_importance\n",
    "            elif name in [\"SVM with Linear Kernel\", \"Logistic Regression\"]:\n",
    "                feature_selector = feature_selection_linear\n",
    "            else:\n",
    "                feature_selector = feature_selection_tree_importances\n",
    "\n",
    "            selected_features = feature_selector(X_cv, y_cv, clf)\n",
    "\n",
    "            X_cv_fs = X_cv.iloc[:, selected_features]\n",
    "            X_val_fs = X_val.iloc[:, selected_features]\n",
    "\n",
    "            clf.fit(X_cv_fs, y_cv)\n",
    "            y_pred = clf.predict(X_val_fs)\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            precision = precision_score(y_val, y_pred, average=average)\n",
    "            recall = recall_score(y_val, y_pred, average=average)\n",
    "            f1 = f1_score(y_val, y_pred, average=average)\n",
    "            f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "            if name not in results:\n",
    "                results[name] = {\n",
    "                    \"accuracy\": [],\n",
    "                    \"precision\": [],\n",
    "                    \"recall\": [],\n",
    "                    \"f1_score\": [],\n",
    "                    \"f1_macro\": [],\n",
    "                    \"selected_features\": [],\n",
    "                    \"correctly_labeled\": [],\n",
    "                    \"mislabeled\": [],\n",
    "                }\n",
    "\n",
    "            results[name][\"accuracy\"].append(acc)\n",
    "            results[name][\"precision\"].append(precision)\n",
    "            results[name][\"recall\"].append(recall)\n",
    "            results[name][\"f1_score\"].append(f1)\n",
    "            results[name][\"f1_macro\"].append(f1_macro)\n",
    "            results[name][\"selected_features\"].append(selected_features)\n",
    "\n",
    "            correct_mask = y_val == y_pred\n",
    "            num_correctly_labeled = correct_mask.sum()\n",
    "            num_mislabeled = (~correct_mask).sum()\n",
    "\n",
    "            results[name][\"correctly_labeled\"].append(num_correctly_labeled)\n",
    "            results[name][\"mislabeled\"].append(num_mislabeled)\n",
    "            \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "all_runs_class_features = []\n",
    "\n",
    "for _ in range(20):\n",
    "    X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "\n",
    "    res = run_training_loop_features(\n",
    "        X_train_resampled,\n",
    "        y_train_resampled,\n",
    "        class_weight=\"balanced\",\n",
    "        average=None,\n",
    "    )\n",
    "    \n",
    "    res_class_features = run_class_feature_selection(\n",
    "        X_train_resampled,\n",
    "        y_train_resampled,\n",
    "        class_weight=\"balanced\",\n",
    "        average=None,\n",
    "    )\n",
    "    all_runs.append(res)\n",
    "    all_runs_class_features.append(res_class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_regression_feature_importances(results_list, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Aggregates class feature importances and plots them for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - results_list: List of dictionaries containing results from multiple runs.\n",
    "    - feature_names: List of feature names.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Aggregating class feature importances for Logistic Regression\n",
    "    class_feature_importances = []\n",
    "\n",
    "    for result in results_list:\n",
    "        if \"Logistic Regression\" in result:\n",
    "            class_feature_importances.extend(result[\"Logistic Regression\"][\"class_features\"])\n",
    "\n",
    "    # Stacking all coefficients to find max absolute value for normalization\n",
    "\n",
    "    stacked_coefs = np.vstack(class_feature_importances)\n",
    "    \n",
    "    # Calculate the maximum absolute coefficient for each feature\n",
    "    max_abs_coefs = np.max(np.abs(stacked_coefs), axis=0)\n",
    "\n",
    "    # Normalize the coefficients by the maximum absolute value for each feature\n",
    "    normalized_importances = []\n",
    "    for coefs in class_feature_importances:\n",
    "        normalized_coefs = np.abs(coefs) / max_abs_coefs\n",
    "        normalized_importances.append(normalized_coefs)\n",
    "\n",
    "    # Averaging the normalized coefficients\n",
    "    averaged_importances = np.mean(np.stack(normalized_importances), axis=0)\n",
    "\n",
    "    # Creating a DataFrame for easier plotting\n",
    "    data = []\n",
    "    for class_idx, feature_coefs in enumerate(averaged_importances):\n",
    "        for feature_idx, coef in enumerate(feature_coefs):\n",
    "            data.append({\n",
    "                'Class': class_names[class_idx],\n",
    "                'Feature': feature_names[feature_idx],\n",
    "                'Importance': coef\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plotting the feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Feature', y='Importance', hue='Class', data=df)\n",
    "    plt.title('Logistic Regression: Feature Importances per Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Class')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_logistic_regression_feature_importances2(results_list, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Aggregates class feature importances and plots them as a heatmap for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - results_list: List of dictionaries containing results from multiple runs.\n",
    "    - feature_names: List of feature names.\n",
    "    - class_names: List of class names.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Aggregating class feature importances for Logistic Regression\n",
    "    class_feature_importances = []\n",
    "\n",
    "    for result in results_list:\n",
    "        if \"Logistic Regression\" in result:\n",
    "            class_feature_importances.extend(result[\"Logistic Regression\"][\"class_features\"])\n",
    "\n",
    "    # Stacking all coefficients to find max absolute value for normalization\n",
    "    stacked_coefs = np.vstack(class_feature_importances)\n",
    "    \n",
    "    # Calculate the maximum absolute coefficient for each feature\n",
    "    max_abs_coefs = np.max(np.abs(stacked_coefs), axis=0)\n",
    "\n",
    "    # Normalize the coefficients by the maximum absolute value for each feature\n",
    "    normalized_importances = []\n",
    "    for coefs in class_feature_importances:\n",
    "        normalized_coefs = np.abs(coefs) / max_abs_coefs\n",
    "        normalized_importances.append(normalized_coefs)\n",
    "\n",
    "    # Averaging the normalized coefficients\n",
    "    averaged_importances = np.mean(np.stack(normalized_importances), axis=0)\n",
    "\n",
    "    # Creating a DataFrame for easier plotting\n",
    "    data = np.zeros((len(feature_names), len(class_names)))\n",
    "    for class_idx, feature_coefs in enumerate(averaged_importances):\n",
    "        for feature_idx, coef in enumerate(feature_coefs):\n",
    "            data[feature_idx, class_idx] = coef\n",
    "\n",
    "    df = pd.DataFrame(data, columns=class_names, index=feature_names)\n",
    "\n",
    "    # Plotting the feature importances as a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Logistic Regression: Feature Importances per Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "plot_logistic_regression_feature_importances2(all_runs_class_features, X_train.columns, sorted(set(y.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_plot_results(results_list):\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple runs and plots the metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - results_list: List of dictionaries containing results from multiple runs.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Aggregating the results\n",
    "    aggregated_results = {}\n",
    "    for result in results_list:\n",
    "        for clf, metrics in result.items():\n",
    "            if clf not in aggregated_results:\n",
    "                aggregated_results[clf] = {\n",
    "                    'accuracy': [],\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1_score': [],\n",
    "                    'f1_macro': [],\n",
    "                    'selected_features': [],\n",
    "                    'correctly_labeled': [],\n",
    "                    'mislabeled': []\n",
    "                }\n",
    "            aggregated_results[clf]['accuracy'].extend(metrics['accuracy'])\n",
    "            aggregated_results[clf]['precision'].extend([np.mean(p) for p in metrics['precision']])\n",
    "            aggregated_results[clf]['recall'].extend([np.mean(r) for r in metrics['recall']])\n",
    "            aggregated_results[clf]['f1_score'].extend([np.mean(f) for f in metrics['f1_score']])\n",
    "            aggregated_results[clf]['f1_macro'].extend(metrics['f1_macro'])\n",
    "            aggregated_results[clf]['selected_features'].extend(metrics['selected_features'])\n",
    "            aggregated_results[clf]['correctly_labeled'].extend(metrics['correctly_labeled'])\n",
    "            aggregated_results[clf]['mislabeled'].extend(metrics['mislabeled'])\n",
    "\n",
    "    # Converting to DataFrame for easier plotting\n",
    "    data = []\n",
    "    for clf, metrics in aggregated_results.items():\n",
    "        for i in range(len(metrics['accuracy'])):\n",
    "            data.append({\n",
    "                'Classifier': clf,\n",
    "                'Accuracy': metrics['accuracy'][i],\n",
    "                'Precision': metrics['precision'][i],\n",
    "                'Recall': metrics['recall'][i],\n",
    "                'F1 Score': metrics['f1_score'][i],\n",
    "                'F1 Macro': metrics['f1_macro'][i],\n",
    "                'Correctly Labeled': metrics['correctly_labeled'][i],\n",
    "                'Mislabeled': metrics['mislabeled'][i]\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plotting metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Classifier', y='Accuracy', data=df)\n",
    "    plt.title('Accuracy across different classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Classifier', y='F1 Score', data=df)\n",
    "    plt.title('F1 Score across different classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Classifier', y='Precision', data=df)\n",
    "    plt.title('Precision across different classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Classifier', y='Recall', data=df)\n",
    "    plt.title('Recall across different classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Classifier', y='F1 Macro', data=df)\n",
    "    plt.title('F1 Macro across different classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize correctly and mislabeled counts\n",
    "    labeling_data = df.groupby('Classifier').agg({\n",
    "        'Correctly Labeled': 'mean',\n",
    "        'Mislabeled': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Plotting correctly vs mislabeled observations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Classifier', y='Correctly Labeled', data=labeling_data, color='b', label='Correctly Labeled')\n",
    "    sns.barplot(x='Classifier', y='Mislabeled', data=labeling_data, color='r', label='Mislabeled', bottom=labeling_data['Correctly Labeled'])\n",
    "    plt.title('Correctly vs Mislabeled Observations across Classifiers')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizing feature selection\n",
    "    feature_names = X_train.columns\n",
    "    selected_features_data = []\n",
    "    for clf, metrics in aggregated_results.items():\n",
    "        for features in metrics['selected_features']:\n",
    "            selected_features_data.append({\n",
    "                'Classifier': clf,\n",
    "                **{feature_names[i]: features[i] for i in range(len(features))}\n",
    "            })\n",
    "\n",
    "    selected_features_df = pd.DataFrame(selected_features_data)\n",
    "\n",
    "    # Plotting heatmap of selected features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(selected_features_df.groupby('Classifier').mean(), annot=True, cmap='viridis')\n",
    "    plt.title('Feature selection across classifiers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "aggregate_and_plot_results(all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_importances_for_misclassified(X_train, y_train, class_weight=\"balanced\"):\n",
    "    feature_importances_correct = []\n",
    "    feature_importances_misclassified = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "        clf = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf.fit(X_cv, y_cv)\n",
    "\n",
    "        # Identify misclassified observations\n",
    "        y_pred = clf.predict(X_val)\n",
    "        mislabeled = y_val != y_pred\n",
    "\n",
    "        # Subset data for correctly labeled and mislabeled observations\n",
    "        X_correct = X_val[~mislabeled]\n",
    "        y_correct = y_val[~mislabeled]\n",
    "\n",
    "        X_misclassified = X_val[mislabeled]\n",
    "        y_misclassified = y_val[mislabeled]\n",
    "\n",
    "        # Check the size of correctly classified and misclassified sets\n",
    "        if len(X_misclassified) > 0:  # Ensure there are misclassified samples\n",
    "            # Calculate permutation importance for correctly labeled observations\n",
    "            perm_importance_correct = permutation_importance(clf, X_correct, y_correct, n_repeats=10, random_state=0)\n",
    "            feature_importances_correct.append(perm_importance_correct.importances_mean)\n",
    "\n",
    "            # Calculate permutation importance for misclassified observations\n",
    "            perm_importance_misclassified = permutation_importance(clf, X_misclassified, y_misclassified, n_repeats=10, random_state=0)\n",
    "            feature_importances_misclassified.append(perm_importance_misclassified.importances_mean)\n",
    "\n",
    "    # Convert lists to arrays for easier analysis\n",
    "    feature_importances_correct = np.array(feature_importances_correct)\n",
    "    feature_importances_misclassified = np.array(feature_importances_misclassified)\n",
    "\n",
    "    return feature_importances_correct, feature_importances_misclassified\n",
    "\n",
    "f_corrs, f_misses = [], []\n",
    "\n",
    "for _ in range(40):\n",
    "    X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "    f_corr, f_miss = get_importances_for_misclassified(X_train_resampled, y_train_resampled, class_weight=\"balanced\")\n",
    "    f_corrs.extend(f_corr)\n",
    "    f_misses.extend(f_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataframes\n",
    "df_corr = pd.DataFrame(f_corr, columns=X_train.columns)\n",
    "df_miss = pd.DataFrame(f_miss, columns=X_train.columns)\n",
    "\n",
    "# Create figure with two subfigures\n",
    "fig, axs = plt.subplots(1, 2, figsize=(24, 8))\n",
    "\n",
    "# Subfigure 1: Correctly classified observations\n",
    "sns.barplot(data=df_corr, ax=axs[0])\n",
    "axs[0].set_title('Feature importances for correctly classified observations')\n",
    "axs[0].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "# Subfigure 2: Misclassified observations\n",
    "sns.barplot(data=df_miss, ax=axs[1])\n",
    "axs[1].set_title('Feature importances for misclassified observations')\n",
    "axs[1].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_classifiers(class_weight: str):\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight=class_weight),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight=class_weight),\n",
    "    }\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "def add_random_features(X, n_features=10):\n",
    "    X = X.copy()\n",
    "    for i in range(n_features):\n",
    "        X[f\"random_{i}\"] = np.random.rand(X.shape[0])\n",
    "    return X\n",
    "\n",
    "def add_related_features(X, n_features=10):\n",
    "    X = X.copy()\n",
    "    initial_X_columns = X.columns\n",
    "    for i in range(n_features):\n",
    "        feature = random.choice(initial_X_columns)\n",
    "        X[f\"related_{feature}_{i}\"] = X[feature]  + np.random.rand()  \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_runs_c = {1: [], 5: [], 20: [], 100: []}\n",
    "all_runs_c_corr = {1: [], 5: [], 20: [], 100: []}\n",
    "all_runs_class_features_c = []\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    for features in [1, 5, 20, 100]:\n",
    "        X_train_temp = add_random_features(X_train, features)\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = resample(X_train_temp, y_train)\n",
    "\n",
    "        res = run_training_loop_features(\n",
    "            X_train_resampled,\n",
    "            y_train_resampled,\n",
    "            class_weight=\"balanced\",\n",
    "            average=None,\n",
    "        )\n",
    "        \n",
    "        res_class_features = run_class_feature_selection(\n",
    "            X_train_resampled,\n",
    "            y_train_resampled,\n",
    "            class_weight=\"balanced\",\n",
    "            average=None,\n",
    "        )\n",
    "        res[\"feature_names\"] = X_train_temp.columns\n",
    "        all_runs_c[features].append(res)\n",
    "        \n",
    "        all_runs_class_features_c.append(res_class_features)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # correlated features\n",
    "        X_train_temp = add_related_features(X_train, features)\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = resample(X_train_temp, y_train)\n",
    "\n",
    "        res = run_training_loop_features(\n",
    "            X_train_resampled,\n",
    "            y_train_resampled,\n",
    "            class_weight=\"balanced\",\n",
    "            average=None,\n",
    "        )\n",
    "        \n",
    "        res_class_features = run_class_feature_selection(\n",
    "            X_train_resampled,\n",
    "            y_train_resampled,\n",
    "            class_weight=\"balanced\",\n",
    "            average=None,\n",
    "        )\n",
    "        res[\"feature_names\"] = X_train_temp.columns\n",
    "        all_runs_c_corr[features].append(res)\n",
    "        \n",
    "        # all_runs_class_features_c.append(res_class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_plot_results(result_data, class_names):\n",
    "    \n",
    "    \n",
    "    \n",
    "    aggregated_results = {\n",
    "        'n_features': [],\n",
    "        'Classifier': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'F1 Macro': [],\n",
    "        'Correctly Labeled': [],\n",
    "        'Mislabeled': []\n",
    "    }\n",
    "    \n",
    "    for n_features, results_list in result_data.items():\n",
    "        # Aggregating the results\n",
    "        for result in results_list:\n",
    "            feature_names = result['feature_names']\n",
    "            for clf, metrics in result.items():\n",
    "                if clf == \"feature_names\":\n",
    "                    continue\n",
    "                # Extend n_features\n",
    "                aggregated_results['n_features'].extend([n_features] * len(metrics['accuracy']))\n",
    "                # Extend classifier\n",
    "                aggregated_results['Classifier'].extend([clf] * len(metrics['accuracy']))\n",
    "                # Extend other metrics\n",
    "                aggregated_results['Accuracy'].extend(metrics['accuracy'])\n",
    "                aggregated_results['Precision'].extend([np.mean(p) for p in metrics['precision']])\n",
    "                aggregated_results['Recall'].extend([np.mean(r) for r in metrics['recall']])\n",
    "                # aggregated_results['F1 Score'].extend([np.mean(f) for f in metrics['f1_score']])\n",
    "                aggregated_results['F1 Score'].extend(metrics['f1_score'])\n",
    "                aggregated_results['F1 Macro'].extend(metrics['f1_macro'])\n",
    "                aggregated_results['Correctly Labeled'].extend(metrics['correctly_labeled'])\n",
    "                aggregated_results['Mislabeled'].extend(metrics['mislabeled'])\n",
    "                \n",
    "\n",
    "    # Converting to DataFrame for easier plotting\n",
    "    df = pd.DataFrame(aggregated_results)\n",
    "    \n",
    "    f1_expanded = []\n",
    "    for i, f1_scores in enumerate(aggregated_results['F1 Score']):\n",
    "        for class_idx, score in enumerate(f1_scores):\n",
    "            f1_expanded.append({\n",
    "                'n_features': aggregated_results['n_features'][i],\n",
    "                'Classifier': aggregated_results['Classifier'][i],\n",
    "                'Class Index': class_idx,\n",
    "                'F1 Score': score,\n",
    "            })\n",
    "\n",
    "    f1_expanded = []\n",
    "    for i, f1_scores in enumerate(aggregated_results['F1 Score']):\n",
    "        for class_idx, score in enumerate(f1_scores):\n",
    "            f1_expanded.append({\n",
    "                'n_features': aggregated_results['n_features'][i] + len(X_train.columns),\n",
    "                'Classifier': aggregated_results['Classifier'][i],\n",
    "                'Class Index': class_idx,\n",
    "                'Class Name': class_names[class_idx],\n",
    "                'F1 Score': score,\n",
    "            })\n",
    "\n",
    "    df_f1 = pd.DataFrame(f1_expanded)\n",
    "\n",
    "    # Plotting F1 Score as a function of n_features, classifier, and class name in a single plot\n",
    "    g = sns.FacetGrid(df_f1, col=\"Class Name\", col_wrap=3, height=4, sharey=True)\n",
    "    g.map_dataframe(sns.boxplot, x='n_features', y='F1 Score', hue='Classifier')\n",
    "    g.add_legend()\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    g.set_xticklabels(rotation=45)\n",
    "\n",
    "    # Set overall title\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    g.fig.suptitle('F1 Score across different classifiers and classes')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Macro']\n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(x='n_features', y=metric, hue='Classifier', data=df)\n",
    "        plt.title(f'{metric} across different classifiers')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_results = []\n",
    "    for n_features, results_list in result_data.items():\n",
    "        for res in results_list:\n",
    "            all_results.append((n_features, res))\n",
    "\n",
    "    aggregated_results = {}\n",
    "    for n_features, results_list in result_data.items():\n",
    "        aggregated_features_data = []\n",
    "        for res in results_list:\n",
    "            feature_names = res['feature_names']\n",
    "            for clf, metrics in res.items():\n",
    "                if clf == \"feature_names\":\n",
    "                    continue\n",
    "                for features in metrics['selected_features']:\n",
    "                    aggregated_features_data.append({\n",
    "                        'Classifier': clf,\n",
    "                        **{feature_names[i]: features[i] for i in range(len(features))}\n",
    "                    })\n",
    "        aggregated_results[n_features] = pd.DataFrame(aggregated_features_data)\n",
    "\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Flatten the axes array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Counter for the subplot index\n",
    "    subplot_idx = 0\n",
    "\n",
    "    for n_features, aggregated_df in aggregated_results.items():\n",
    "        # Calculate the mean selection for each classifier\n",
    "        mean_df = aggregated_df.groupby('Classifier').mean()\n",
    "\n",
    "        # Plotting heatmap in the current subplot\n",
    "        sns.heatmap(mean_df, cmap='viridis', ax=axs[subplot_idx], cbar=True)\n",
    "        axs[subplot_idx].set_title(f'Feature selection (num fetures={n_features + len(X_train.columns)})')\n",
    "        axs[subplot_idx].set_xticklabels(axs[subplot_idx].get_xticklabels(), rotation=90)\n",
    "        axs[subplot_idx].set_yticklabels(axs[subplot_idx].get_yticklabels(), rotation=0)\n",
    "\n",
    "        # Move to the next subplot\n",
    "        subplot_idx += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "aggregate_and_plot_results(all_runs_c, sorted(set(y.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_and_plot_results(all_runs_c_corr, sorted(set(y.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_classifiers(class_weight):\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight=class_weight),\n",
    "        \"SVM with RBF Kernel\": SVC(kernel=\"rbf\", class_weight=class_weight, probability=True),\n",
    "        \"SVM with Linear Kernel\": SVC(kernel=\"linear\", class_weight=class_weight, probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight=class_weight),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(class_weight=class_weight),\n",
    "        \"k-NN\": KNeighborsClassifier(n_neighbors=3),\n",
    "    }\n",
    "    \n",
    "    return classifiers\n",
    "    \n",
    "\n",
    "def find_common_misspreds(X_train, y_train, X_test, y_test, class_weight=\"balanced\", n_splits=10, n_resamples=10):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    all_results = defaultdict(lambda: {\n",
    "        \"misclassified\": [], \"probabilities\": [], \"predictions\": [], \"correct\": [], \"index\": [],\n",
    "        \"test_index\": [], \"test_probabilities\": [], \"test_predictions\": [], \"test_correct\": []\n",
    "    })\n",
    "    \n",
    "    for _ in range(n_resamples):\n",
    "        classifiers= create_classifiers(class_weight)\n",
    "        results = {name: { \"probabilities\": [], \"predictions\": [], \"correct\": [], \"index\": []} for name in classifiers.keys()}\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "        \n",
    "        for train_index, val_index in skf.split(X_train_resampled, y_train_resampled):\n",
    "            X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "            classifiers = create_classifiers(class_weight)\n",
    "            \n",
    "            for name, clf in classifiers.items():\n",
    "                clf.fit(X_cv, y_cv)\n",
    "                \n",
    "                y_pred = clf.predict(X_val)\n",
    "                probas = clf.predict_proba(X_val)\n",
    "                misclassified = y_val != y_pred\n",
    "                \n",
    "                misclassified = misclassified[misclassified]\n",
    "                \n",
    "                # results[name][\"misclassified\"].append(misclassified.index)\n",
    "                results[name][\"probabilities\"].append(probas)\n",
    "                results[name][\"predictions\"].append(y_pred)\n",
    "                results[name][\"correct\"].append(y_val)\n",
    "                results[name][\"index\"].append(y_val.index)\n",
    "                \n",
    "        \n",
    "        for name in results:\n",
    "            # all_results[name][\"misclassified\"].extend(results[name][\"misclassified\"])\n",
    "            all_results[name][\"probabilities\"].extend(results[name][\"probabilities\"])\n",
    "            all_results[name][\"predictions\"].extend(results[name][\"predictions\"])\n",
    "            all_results[name][\"correct\"].extend(results[name][\"correct\"])\n",
    "            all_results[name][\"index\"].extend(results[name][\"index\"])\n",
    "    \n",
    "        # After all cross-validation runs, fit classifiers on full training data and predict on test set\n",
    "        classifiers = create_classifiers(class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "            clf.fit(X_train_resampled, y_train_resampled)\n",
    "            test_pred = clf.predict(X_test)\n",
    "            test_probas = clf.predict_proba(X_test)\n",
    "            test_misclassified = y_test != test_pred\n",
    "            \n",
    "            test_misclassified = test_misclassified[test_misclassified]\n",
    "            \n",
    "            # all_results[name][\"test_misclassified\"].append(test_misclassified.index)\n",
    "            all_results[name][\"test_probabilities\"].append(test_probas)\n",
    "            all_results[name][\"test_predictions\"].append(test_pred)\n",
    "            all_results[name][\"test_correct\"].append(y_test)\n",
    "            all_results[name][\"test_index\"].append(y_test.index)\n",
    "    \n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "results = find_common_misspreds(X_train, y_train, X_test, y_test, class_weight=\"balanced\", n_resamples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_cv = {}\n",
    "for model, result in results.items():\n",
    "    probabilities = np.concatenate(result[\"probabilities\"]).tolist()\n",
    "    predictions = np.concatenate(result[\"predictions\"])\n",
    "    correct = np.concatenate(result[\"correct\"])\n",
    "    index = np.concatenate(result[\"index\"])\n",
    "    df_cv[f\"probabilities_{model}\"] = probabilities\n",
    "    df_cv[f\"predictions_{model}\"] = predictions\n",
    "df_cv[f\"correct\"] = correct\n",
    "df_cv[f\"index\"] = index\n",
    "\n",
    "\n",
    "\n",
    "df_test = {}\n",
    "for model, result in results.items():\n",
    "    probabilities = np.concatenate(result[\"test_probabilities\"]).tolist()\n",
    "    predictions = np.concatenate(result[\"test_predictions\"])\n",
    "    correct = np.concatenate(result[\"test_correct\"])\n",
    "    index = np.concatenate(result[\"test_index\"])\n",
    "    df_test[f\"probabilities_{model}\"] = probabilities\n",
    "    df_test[f\"predictions_{model}\"] = predictions\n",
    "df_test[\"correct\"] = correct\n",
    "df_test[\"index\"] = index\n",
    "    \n",
    "\n",
    "    \n",
    "df_cv = pd.DataFrame(df_cv)\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_misspredictions(row, threshold=0.8):\n",
    "    # return True if all models disagree with the correct label\n",
    "    first_model = list(results.keys())[0]\n",
    "    first_prediction = row[f\"predictions_{first_model}\"]\n",
    "\n",
    "    if (all(first_prediction == row[f\"predictions_{model}\"] for model in results) and\n",
    "        all(max(row[f\"probabilities_{model}\"]) > threshold for model in results) and\n",
    "        first_prediction != row[\"correct\"]):\n",
    "        return True\n",
    "    return False\n",
    "    # return all(row[f\"predictions_{model}\"] != row[\"correct\"] for model in results) and all(max(row[f\"probabilities_{model}\"]) > threshold for model in results)\n",
    "\n",
    "\n",
    "def find_low_confidence(row, threshold=0.7):\n",
    "    return all(max(row[f\"probabilities_{model}\"]) < threshold for model in results if f\"probabilities_{model}\" in row)\n",
    "\n",
    "def find_high_confidence(row, threshold=0.8):\n",
    "    return all(max(row[f\"probabilities_{model}\"]) > threshold for model in results if f\"probabilities_{model}\" in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_cv.apply(find_common_misspredictions, axis=1)\n",
    "mispreds = df_cv[indices]\n",
    "\n",
    "mispreds = mispreds.drop_duplicates(subset='index', keep='first')\n",
    "mispreds\n",
    "\n",
    "\n",
    "indices_test = df_test.apply(find_common_misspredictions, axis=1)\n",
    "mispreds_test = df_test[indices_test]\n",
    "\n",
    "mispreds_test = mispreds_test.drop_duplicates(subset='index', keep='first')\n",
    "# let probability be the max probability, so take the max for each model\n",
    "for model in results:\n",
    "    mispreds_test[f\"probabilities_{model}\"] = mispreds_test[f\"probabilities_{model}\"].apply(max).round(2)\n",
    "mispreds_test.drop(columns=[\"index\"], inplace=True)\n",
    "mispreds_test[\"prediction\"] = mispreds_test.apply(lambda x: x[\"predictions_Logistic Regression\"], axis=1)\n",
    "mispreds_test.drop(columns=[col for col in mispreds_test.columns if \"predictions\" in col], inplace=True)\n",
    "\n",
    "for model in results:\n",
    "    mispreds[f\"probabilities_{model}\"] = mispreds[f\"probabilities_{model}\"].apply(max).round(2)\n",
    "mispreds.drop(columns=[\"index\"], inplace=True)\n",
    "# add a single label named prediction and remove the individual model predictions\n",
    "mispreds[\"prediction\"] = mispreds.apply(lambda x: x[\"predictions_Logistic Regression\"], axis=1)\n",
    "mispreds.drop(columns=[col for col in mispreds.columns if \"predictions\" in col], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of potentially mislabeled in cv: \", len(mispreds))\n",
    "print(\"Number of potentially mislabeled in test: \", len(mispreds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispreds_test.to_csv(\"mispreds_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(set(y.unique()))\n",
    "class_names_map = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "\n",
    "def find_same_set(row, k=2):\n",
    "    # check if the all models have the highest probability for the same k classes\n",
    "    top_k = []\n",
    "    for model in results:\n",
    "        if f\"probabilities_{model}\" not in row:\n",
    "            continue\n",
    "        top_k.append(np.argsort(row[f\"probabilities_{model}\"])[-k:])\n",
    "    return len(set.intersection(*map(set, top_k))) == k\n",
    "\n",
    "def add_set_classes(row, k=2):\n",
    "    top_k = []\n",
    "    for model in results:\n",
    "        if f\"probabilities_{model}\" not in row:\n",
    "            continue\n",
    "        top_k.append(np.argsort(row[f\"probabilities_{model}\"])[-k:])\n",
    "    \n",
    "    return [class_names_map[i] for i in set.intersection(*map(set, top_k))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR SVARAR JAG P CONFIDENCE OCH SETS\n",
    "\n",
    "\n",
    "def majority_prediction_matches(df):\n",
    "    \"\"\"\n",
    "    Function to check if the majority prediction of the models \n",
    "    matches the correct label for each row in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df: pandas.DataFrame\n",
    "        The input DataFrame containing prediction columns and the correct labels.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with an additional column 'majority_correct' indicating\n",
    "        if the majority prediction matches the correct label.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract the prediction columns\n",
    "    prediction_columns = [col for col in df.columns if col.startswith('predictions_')]\n",
    "\n",
    "    def check_majority_prediction(row):\n",
    "        # Step 2: Compute the mode of predictions\n",
    "        mode_predictions = row[prediction_columns].mode()\n",
    "        \n",
    "        # If there's no majority or multiple modes, it can return multiple values, so check the first\n",
    "        if not mode_predictions.empty:\n",
    "            majority_prediction = mode_predictions.iloc[0]\n",
    "            \n",
    "            # Step 3: Compare the mode with the correct label\n",
    "            if majority_prediction == row['correct']:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Apply the function to each row in the DataFrame\n",
    "    \n",
    "    df['majority_correct'] = df.apply(check_majority_prediction, axis=1)\n",
    "    # drop all rows where the majority prediction is not correct\n",
    "    df = df[df[\"majority_correct\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def return_df_with_sets(df):\n",
    "    df_no_dt = df.drop(columns=[\"predictions_Decision Tree\", \"probabilities_Decision Tree\"])\n",
    "    \n",
    "    low_conf = df_no_dt.apply(find_low_confidence, axis=1)\n",
    "    low_conf = df_no_dt[low_conf]\n",
    "    \n",
    "    sets = low_conf[low_conf.apply(find_same_set, axis=1)]\n",
    "    sets = sets.drop_duplicates(subset='index', keep='first')\n",
    "    \n",
    "    sets[\"classes\"] = sets.apply(add_set_classes, axis=1)\n",
    "    return sets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_test_sets = return_df_with_sets(df_test)\n",
    "df_test_sets\n",
    "\n",
    "df_cv_sets = return_df_with_sets(df_cv)\n",
    "df_cv_sets\n",
    "\n",
    "print(\"Test set\", len(df_test_sets), \"total\", len(df_test[\"index\"].unique()), \"proportion\", len(df_test_sets) / len(df_test[\"index\"].unique()))\n",
    "print(\"CV set\", len(df_cv_sets),  \"total\", len(df_cv[\"index\"].unique()), \"proportion\", len(df_cv_sets) / len(df_cv[\"index\"].unique()))\n",
    "\n",
    "\n",
    "temp_df = df_test_sets.copy()\n",
    "# save only the top 2 probabilities for each model and correct label and the new set\n",
    "for model in results:\n",
    "    if f\"probabilities_{model}\" not in temp_df.columns:\n",
    "        continue\n",
    "    temp_df[f\"probabilities_{model}\"] = temp_df[f\"probabilities_{model}\"].apply(lambda x: [class_names[i] for i in np.argsort(x)[-2:]])\n",
    "\n",
    "temp_df.drop(columns=[\"index\"], inplace=True)\n",
    "temp_df[:5].to_csv(\"test_set.csv\")\n",
    "\n",
    "high_confidence_test = df_test.apply(find_high_confidence, axis=1)\n",
    "high_confidence_test = df_test[high_confidence_test]\n",
    "high_confidence_test = high_confidence_test.drop_duplicates(subset='index', keep='first')\n",
    "high_confidence_test\n",
    "\n",
    "high_confidence_cv = df_cv.apply(find_high_confidence, axis=1)\n",
    "high_confidence_cv = df_cv[high_confidence_cv]\n",
    "high_confidence_cv = high_confidence_cv.drop_duplicates(subset='index', keep='first')\n",
    "high_confidence_cv\n",
    "\n",
    "high_confidence_test = majority_prediction_matches(high_confidence_test)\n",
    "\n",
    "high_confidence_cv = majority_prediction_matches(high_confidence_cv)\n",
    "\n",
    "print(\"Test high confidence\", len(high_confidence_test), \"total\", len(df_test[\"index\"].unique()), \"proportion\", len(high_confidence_test) / len(df_test[\"index\"].unique()))\n",
    "print(\"CV high confidence\", len(high_confidence_cv),  \"total\", len(df_cv[\"index\"].unique()), \"proportion\", len(high_confidence_cv) / len(df_cv[\"index\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "\n",
    "# pca = KernelPCA(n_components=6, kernel='rbf')\n",
    "\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_pca = pd.DataFrame(X_pca, index=X_train.index)\n",
    "\n",
    "clusters = KMeans(n_clusters=7, random_state=42).fit(X_pca)\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_pca = pca.fit_transform(X_train)\n",
    "# X_pca = tsne.fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "class_names_map = {name: idx for idx, name in enumerate(sorted(set(y.unique())))}\n",
    "\n",
    "class_names = [class_names_map[name] for name in y_train]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca.iloc[:, 0], X_pca.iloc[:, 1], c=class_names, cmap='viridis_r')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=class_names_map.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index in random.sample(mispreds[\"index\"].tolist(), k=5):\n",
    "    plt.scatter(X_pca.loc[index, 0], X_pca.loc[index, 1], marker='x', s=100, c='red')\n",
    "    predicted = df_cv[df_cv[\"index\"] == index][\"predictions_Logistic Regression\"].values[0]\n",
    "    plt.annotate(predicted, (X_pca.loc[index, 0], X_pca.loc[index, 1]), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=12, color='black')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
