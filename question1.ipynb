{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_classifiers(class_weight: str = 'balanced'):\n",
    "    return {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight=class_weight),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight=class_weight),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight=class_weight),\n",
    "    \"Support Vector Machine\": SVC(class_weight=class_weight, probability=True),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"K-Nearest Neighbors Rigid\": KNeighborsClassifier(n_neighbors=50)\n",
    "    }\n",
    "\n",
    "def run_training_loop(X_train, y_train, X_test, y_test, task=\"a\", class_weight: str = 'balanced', verbose: bool = False,  average='micro'):\n",
    "    \n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = create_classifiers(class_weight=class_weight)\n",
    "\n",
    "    # Stratified K-Fold Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    # Metrics storage\n",
    "    results = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []} for name in classifiers.keys()}\n",
    "    n_classes = len(np.unique(y_test))  \n",
    "    \n",
    "    aggregate_conf_matrix_train = {name: np.zeros((n_classes, n_classes)) for name in classifiers.keys()}\n",
    "    aggregate_conf_matrix_test = {name: np.zeros((n_classes, n_classes)) for name in classifiers.keys()}\n",
    "    \n",
    "\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # re init classifiers to prevent data leakage from previous fold\n",
    "        classifiers = create_classifiers(class_weight=class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "            # Train the classifier\n",
    "            clf.fit(X_cv, y_cv)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = clf.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            precision = precision_score(y_val, y_pred, average=average)\n",
    "            recall = recall_score(y_val, y_pred, average=average)\n",
    "            f1 = f1_score(y_val, y_pred, average=average)\n",
    "            \n",
    "            conf_matrix = confusion_matrix(y_val, y_pred, labels=np.unique(y_test))\n",
    "            aggregate_conf_matrix_train[name] += conf_matrix\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Store metrics\n",
    "            results[name]['accuracy'].append(acc)\n",
    "            results[name]['precision'].append(precision)\n",
    "            results[name]['recall'].append(recall)\n",
    "            results[name]['f1_score'].append(f1)\n",
    "\n",
    "        if verbose:\n",
    "            # Display results\n",
    "            for name, metrics in results.items():\n",
    "                print(f\"Classifier: {name}\")\n",
    "                print(f\"  Accuracy:  {np.mean(metrics['accuracy']):.4f} ± {np.std(metrics['accuracy']):.4f}\")\n",
    "                print(f\"  Precision: {np.mean(metrics['precision']):.4f} ± {np.std(metrics['precision']):.4f}\")\n",
    "                print(f\"  Recall:    {np.mean(metrics['recall']):.4f} ± {np.std(metrics['recall']):.4f}\")\n",
    "                # print(f\"  F1 Score:  {np.mean(metrics['f1_score']):.4f} ± {np.std(metrics['f1_score']):.4f}\")\n",
    "                print()\n",
    "\n",
    "            # Detailed classification report for each classifier on the last fold\n",
    "            for name, clf in classifiers.items():\n",
    "                y_pred = clf.predict(X_test)\n",
    "                print(f\"Classifier: {name}\")\n",
    "                print(classification_report(y_cv, y_pred))\n",
    "                print()\n",
    "    \n",
    "    classifiers = create_classifiers(class_weight=class_weight)\n",
    "    results_test = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []} for name in classifiers.keys()}\n",
    "    for name, clf in classifiers.items():\n",
    "        # Train the classifier on the full training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=average)\n",
    "        recall = recall_score(y_test, y_pred, average=average)\n",
    "        f1 = f1_score(y_test, y_pred, average=average)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "        aggregate_conf_matrix_test[name] += conf_matrix\n",
    "        \n",
    "        # Store metrics\n",
    "        results_test[name]['accuracy'].append(acc)\n",
    "        results_test[name]['precision'].append(precision)\n",
    "        results_test[name]['recall'].append(recall)\n",
    "        results_test[name]['f1_score'].append(f1)\n",
    "    \n",
    "    \n",
    "\n",
    "    return results, results_test, aggregate_conf_matrix_train, aggregate_conf_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_variance(X_train, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(\"Sum of the two PCS\", pca.explained_variance_ratio_.sum())\n",
    "    \n",
    "def get_distribution(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "def standard_scale(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return pd.DataFrame(data_scaled, columns=data.columns, index=data.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_boxplots(results, y):\n",
    "    accuracy_data = []\n",
    "    precision_data = []\n",
    "    recall_data = []\n",
    "    f1_data = []\n",
    "    \n",
    "    class_names = sorted(set(y))\n",
    "\n",
    "    for model, metrics in results.items():\n",
    "        # Accuracy\n",
    "        for acc in metrics[\"accuracy\"]:\n",
    "            accuracy_data.append([model, acc])\n",
    "\n",
    "        # Precision and Recall for each class\n",
    "        for fold in range(len(metrics[\"precision\"])):\n",
    "            for class_idx, prec in enumerate(metrics[\"precision\"][fold]):\n",
    "                precision_data.append([model, class_names[class_idx], prec])\n",
    "            for class_idx, rec in enumerate(metrics[\"recall\"][fold]):\n",
    "                recall_data.append([model, class_names[class_idx], rec])\n",
    "            for class_idx, f1 in enumerate(metrics[\"f1_score\"][fold]):\n",
    "                f1_data.append([model, class_names[class_idx], f1])\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    accuracy_df = pd.DataFrame(accuracy_data, columns=[\"Model\", \"Accuracy\"])\n",
    "    precision_df = pd.DataFrame(precision_data, columns=[\"Model\", \"Class\", \"Precision\"])\n",
    "    recall_df = pd.DataFrame(recall_data, columns=[\"Model\", \"Class\", \"Recall\"])\n",
    "    f1_df = pd.DataFrame(f1_data, columns=[\"Model\", \"Class\", \"F1 Score\"])\n",
    "\n",
    "   \n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Boxplot for Accuracy\n",
    "    sns.boxplot(x=\"Model\", y=\"Accuracy\", data=accuracy_df, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('Accuracy Comparison')\n",
    "    axs[0, 0].set_ylim(0, 1)\n",
    "    labels = axs[0, 0].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "\n",
    "    # Boxplot for Precision\n",
    "    sns.boxplot(x=\"Model\", y=\"Precision\", hue=\"Class\", data=precision_df, ax=axs[0, 1])\n",
    "    axs[0, 1].set_title('Precision Comparison by Class')\n",
    "    axs[0, 1].set_ylim(0, 1)\n",
    "    labels = axs[0, 1].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[0, 1].legend(title='Class', bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    # Boxplot for Recall\n",
    "    sns.boxplot(x=\"Model\", y=\"Recall\", hue=\"Class\", data=recall_df, ax=axs[1, 0])\n",
    "    axs[1, 0].set_title('Recall Comparison by Class')\n",
    "    axs[1, 0].set_ylim(0, 1)\n",
    "    labels = axs[1, 0].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[1, 0].legend(title='Class', bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    # Boxplot for F1 Score\n",
    "    sns.boxplot(x=\"Model\", y=\"F1 Score\", hue=\"Class\", data=f1_df, ax=axs[1, 1])\n",
    "    axs[1, 1].set_title('F1 Score Comparison by Class')\n",
    "    axs[1, 1].set_ylim(0, 1)\n",
    "    labels = axs[1, 1].get_xticklabels()\n",
    "    plt.setp(labels, rotation=45)\n",
    "    axs[1, 1].legend(title='Class', bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matricies(aggregate_conf_matrix_train: dict, aggregate_conf_matrix_test: dict, y):\n",
    "    class_names = sorted(set(y))\n",
    "\n",
    "    # Create a figure for train confusion matrices\n",
    "    fig_train, axs_train = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    for i, (model, conf_matrix) in enumerate(aggregate_conf_matrix_train.items()):\n",
    "        sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs_train[i//2, i%2])\n",
    "        axs_train[i//2, i%2].set_title(f\"Train Confusion Matrix - {model}\")\n",
    "        axs_train[i//2, i%2].set_xlabel('Predicted')\n",
    "        axs_train[i//2, i%2].set_ylabel('Actual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a figure for test confusion matrices\n",
    "    fig_test, axs_test = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    for i, (model, conf_matrix) in enumerate(aggregate_conf_matrix_test.items()):\n",
    "        sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs_test[i//2, i%2])\n",
    "        axs_test[i//2, i%2].set_title(f\"Test Confusion Matrix - {model}\")\n",
    "        axs_test[i//2, i%2].set_xlabel('Predicted')\n",
    "        axs_test[i//2, i%2].set_ylabel('Actual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "\n",
    "    merged_dict = {}\n",
    "    # Iterate through each key in dict1\n",
    "    for key in dict1.keys():\n",
    "        if key in dict2:\n",
    "            merged_dict[key] = {}\n",
    "            for subkey in dict1[key]:\n",
    "                # Check if the subkey exists in both dictionaries\n",
    "                if subkey in dict2[key]:\n",
    "                    # For accuracy, just append the list\n",
    "                    if subkey == 'accuracy':\n",
    "                        merged_dict[key][subkey] = dict1[key][subkey] + dict2[key][subkey]\n",
    "                    else:\n",
    "                        # For arrays, append the lists of arrays\n",
    "                        merged_dict[key][subkey] = dict1[key][subkey] + dict2[key][subkey]\n",
    "                else:\n",
    "                    # Handle the case where a subkey exists in dict1 but not in dict2\n",
    "                    merged_dict[key][subkey] = dict1[key][subkey]\n",
    "        else:\n",
    "            # Handle the case where a key exists in dict1 but not in dict2\n",
    "            merged_dict[key] = dict1[key]\n",
    "    # Add any keys from dict2 that are not in dict1\n",
    "    for key in dict2.keys():\n",
    "        if key not in dict1:\n",
    "            merged_dict[key] = dict2[key]\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def run_train_multiple_testing(X, y, task=\"a\", class_weight: str = 'balanced', verbose: bool = False,  average='micro', do_smote=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if do_smote:\n",
    "        smote = SMOTE()\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    results, results_test, aggregated_conf_matrix_train, aggregated_conf_matrix_test  = run_training_loop(X_train, y_train, X_test, y_test, task=task, class_weight=class_weight, verbose=verbose, average=average)\n",
    "    return results, results_test, aggregated_conf_matrix_train, aggregated_conf_matrix_test\n",
    "    # results, results_test = run_training_loop(X_train, y_train, X_test, y_test, task=task, class_weight=class_weight, verbose=verbose, average=average)\n",
    "    # return results, results_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"data/Fish3.txt\", delimiter=\" \")\n",
    "\n",
    "X = data.drop(columns=['Species'])\n",
    "y = data['Species']\n",
    "\n",
    "scaled_data = standard_scale(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print_pca_variance(X, n_components=2)\n",
    "get_distribution(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_train = []\n",
    "all_results_test = []\n",
    "all_conf_matrix_train = {}\n",
    "all_conf_matrix_test = {}\n",
    "for _ in range(20):\n",
    "    results_train_temp, results_test_temp, conf_train, conf_test = run_train_multiple_testing(X, y, task=\"a\", class_weight=\"balanced\", average=None)\n",
    "    all_results_train.append(results_train_temp)\n",
    "    all_results_test.append(results_test_temp)\n",
    "    for key, value in conf_train.items():\n",
    "        if key in all_conf_matrix_train:\n",
    "            all_conf_matrix_train[key] += value\n",
    "        else:\n",
    "            all_conf_matrix_train[key] = value\n",
    "            \n",
    "    for key, value in conf_test.items():\n",
    "        if key in all_conf_matrix_test:\n",
    "            all_conf_matrix_test[key] += value\n",
    "        else:\n",
    "            all_conf_matrix_test[key] = value\n",
    "    \n",
    "\n",
    "results_train = all_results_train[0]\n",
    "results_test = all_results_test[0]\n",
    "\n",
    "for i in range(1, len(all_results_train)):\n",
    "    results_train = merge_dictionaries(results_train, all_results_train[i])\n",
    "    results_test = merge_dictionaries(results_test, all_results_test[i])\n",
    "\n",
    "for key, value in all_conf_matrix_train.items():\n",
    "    all_conf_matrix_train[key] = value / value.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "for key, value in all_conf_matrix_test.items():\n",
    "    all_conf_matrix_test[key] = value / value.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "print(\"successfully merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matricies(all_conf_matrix_train, all_conf_matrix_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(results_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(results_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_train_smote = []\n",
    "all_results_test_smote = []\n",
    "all_conf_matrix_train_smote = {}\n",
    "all_conf_matrix_test_smote = {}\n",
    "for _ in range(20):\n",
    "    results_train_temp_smote, results_test_temp_smote, conf_train_smote, conf_test_smote = run_train_multiple_testing(X, y, task=\"a\", class_weight=None, average=None, do_smote=True)\n",
    "    all_results_train_smote.append(results_train_temp_smote)\n",
    "    all_results_test_smote.append(results_test_temp_smote)\n",
    "    for key, value in conf_train_smote.items():\n",
    "        if key in all_conf_matrix_train_smote:\n",
    "            all_conf_matrix_train_smote[key] += value\n",
    "        else:\n",
    "            all_conf_matrix_train_smote[key] = value\n",
    "            \n",
    "    for key, value in conf_test_smote.items():\n",
    "        if key in all_conf_matrix_test_smote:\n",
    "            all_conf_matrix_test_smote[key] += value\n",
    "        else:\n",
    "            all_conf_matrix_test_smote[key] = value\n",
    "    \n",
    "\n",
    "results_train_smote = all_results_train_smote[0]\n",
    "results_test_smote = all_results_test_smote[0]\n",
    "\n",
    "for i in range(1, len(all_results_train_smote)):\n",
    "    results_train_smote = merge_dictionaries(results_train_smote, all_results_train_smote[i])\n",
    "    results_test_smote = merge_dictionaries(results_test_smote, all_results_test_smote[i])\n",
    "\n",
    "for key, value in all_conf_matrix_train_smote.items():\n",
    "    all_conf_matrix_train_smote[key] = value / value.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "for key, value in all_conf_matrix_test_smote.items():\n",
    "    all_conf_matrix_test_smote[key] = value / value.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "print(\"successfully merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matricies(all_conf_matrix_train_smote, all_conf_matrix_test_smote, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(results_train_smote, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(results_test_smote, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "y_labels = LabelEncoder().fit_transform(y_train)\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "clf.fit(X_train, y_labels)\n",
    "\n",
    "clf.coef_ > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop_features(X_train, y_train, X_test, y_test, optimize_for: str = \"accuracy\", class_weight: str = 'balanced', verbose: bool = False,  average='micro'):\n",
    "    \n",
    "    \n",
    "    # Define classifiers\n",
    "    \n",
    "\n",
    "    # Stratified K-Fold Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    classifiers = create_classifiers(class_weight=class_weight)\n",
    "\n",
    "    # Metrics storage\n",
    "    results = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []} for name in classifiers.keys()}\n",
    "    n_classes = len(np.unique(y_test))  \n",
    "    \n",
    "\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        classifiers = create_classifiers(class_weight=class_weight)\n",
    "        for name, clf in classifiers.items():\n",
    "            # Train the classifier\n",
    "            clf.fit(X_cv, y_cv)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = clf.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            precision = precision_score(y_val, y_pred, average=average)\n",
    "            recall = recall_score(y_val, y_pred, average=average)\n",
    "            f1 = f1_score(y_val, y_pred, average=average)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Store metrics\n",
    "            results[name]['accuracy'].append(acc)\n",
    "            results[name]['precision'].append(precision)\n",
    "            results[name]['recall'].append(recall)\n",
    "            results[name]['f1_score'].append(f1)\n",
    "\n",
    "        \n",
    "    classifiers = create_classifiers(class_weight=class_weight)\n",
    "    results_test = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []} for name in classifiers.keys()}\n",
    "    for name, clf in classifiers.items():\n",
    "        # Train the classifier on the full training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=average)\n",
    "        recall = recall_score(y_test, y_pred, average=average)\n",
    "        f1 = f1_score(y_test, y_pred, average=average)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Store metrics\n",
    "        results_test[name]['accuracy'].append(acc)\n",
    "        results_test[name]['precision'].append(precision)\n",
    "        results_test[name]['recall'].append(recall)\n",
    "        results_test[name]['f1_score'].append(f1)\n",
    "    \n",
    "    \n",
    "\n",
    "    return results, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTA!!! att köra typ selectkbest som ena grejen och köra cross val med avseende på accuracy, och sen f1 för varje klass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = run_training_loop_features(X_train, y_train, X_test, y_test, task=\"a\", class_weight=\"balanced\", verbose=False, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
